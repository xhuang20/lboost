% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/lboost.R
\name{lboost}
\alias{lboost}
\title{Lassoed boost function}
\usage{
lboost(x, y, family = "gaussian", weights = NULL, alpha = 1, nlambda = 100,
  lambda.min.ratio = ifelse(nobs < nvar, 0.01, 1e-04), lambda = NULL,
  standardize = TRUE, intercept = TRUE, thresh = 1e-07, nu = 0.1, bstop = 1000,
  nb = 50, lf = 0, uf = 1)
}
\arguments{
\item{x}{An n by p matrix of variables with n observations and p variables,
either a matrix type or a data frame.}

\item{y}{A n by 1 vector of dependent variable.}

\item{family}{FGP. A character string equal to "gaussian" and no other
character string is accepted.}

\item{weights}{FGP. Weights for each observation. Defaults to a vector of
ones.}

\item{alpha}{FGP. The elasticnet mixing parameter and \eqn{0 \le \alpha \le 1}.}

\item{nlambda}{FGP. The number of lambda values. Defaults to 100.}

\item{lambda.min.ratio}{FGP. Smallest value for \code{lambda} as a fraction
the maximum value of \code{lambda}, \code{lambda.max}.}

\item{lambda}{FGP. A sequence of penalty parameter values. If the value is
\code{NULL}, a sequnce of log-scaled values are generated, similar to those
in the original \code{glmnet} package. A user can also supply a preferred
\code{lambda} sequence. Defaults to \code{NULL} with 100 values.}

\item{standardize}{FGP. Logical value for whether to standardize \code{x} prior
to model fitting. Defaults to \code{TRUE}.}

\item{intercept}{FGP. A logical value for whether to report the intercept in the
\code{coef.lboost} function. Defaults to \code{TRUE}. The estimation always
include an intercept.}

\item{thresh}{FGP. Convergence criterion for coordinate descent. Defaults to \code{1e-7}.}

\item{nu}{The learning rate in boosting and \eqn{0 < \nu < 1}. Defaults to 0.1.}

\item{bstop}{The initial boosting iteration number. This number should be large enough
so that it is larger than the usual stopping criterion in boosting or a multiple of
the stopping criterion. Examples include \code{1000}, \code{5000} or more. Choosing
a very large number will increase the computation cost. This number should also
be increased if one uses a smaller learning rate such as \code{0.01} or \code{0.001}.}

\item{nb}{The number of equally spaced boosting steps on the sequence from \code{1}
to \code{bstop}. There are \code{nb} selected LS-boost estimates (vectors) for each
active set of variables determined by lasso in the first stage.The product,
\code{nlambda} \eqn{*} \code{nb}, is the number of columns of the coefficient matrix
in the output of this function.}

\item{lf}{Lower factor. When multiplied by the AIC-based stopping creterion, the product
determines the first of the \code{nb} selected LS-boost steps . Defaults to \code{0}.}

\item{uf}{Upper factor. When multiplied by the AIC-based stopping criterion, the product
determins the last of the \code{nb} selected LS-boost steps. Defaults to \code{1}.
\code{lf} and \code{uf}, when multiplied by the AIC-based stopping criterion, define
an interval of boosting steps, on which we sample \code{nb} equally-spaced steps. Some
examples of \code{lf} and \code{uf} include \eqn{(0,1), (0,2), (0.2, 1.5),} etc.}
}
\value{
\code{lboost} returns a S3 class "lboost" with the following components:
\item{call}{the function call} \item{beta}{A matrix of coefficient estimates, the first
row of which are the intercept estimates.} \item{x}{the \code{x} data} \item{y}{the \code{y}
data} \item{intercept}{the logical value for \code{intercept}} \item{nlambda}{the value
of \code{nlambda}} \item{stop.num}{a sequnce of AIC-based criteria for LS-boost on each
subset of variables generated by lasso. These numbers will help determine if the choices
of \code{bstop}, \code{lf}, and \code{uf} are approriate.}
}
\description{
This function computes the lassoed boosting estimates in a
linear regression. It is build on the glmnet package and the mboost
package. We use the acronym FGP if a parameter is from the glmnet
package.
}
\details{
This is the main function of the package and it returns the parameter estimates
of lassoed boosting for a linear regression. This is a two-stage procedure. In the first
stage, lasso returns \code{nlambda} subsets of variables, some of which may be the same
and may include all variables. In the second step, least-squares boosting (LS-boost) is
use to spawn coefficient solution path for each subset of variables. The function reports
\code{nb} solution vectors for each subset of variables.

\code{nlambda}, \code{nu}, \code{nb} are all tuning parameters. In addition, \code{lf} and
\code{uf} can also be changed.

The estimation always includes an intercept. When \code{intercept = FALSE}, the
\code{coef.lboost} function will skip returning the intercept estimates and
\code{predict.lbbost} function will exclude the intercept when computing predictions.
}
\examples{
\dontrun{
# An example of using the bodyfat data in R.
library(lboost)
data("bodyfat", package = "TH.data")
x = bodyfat[,!colnames(bodyfat) \%in\% c("DEXfat")]
y = bodyfat[,c("DEXfat")]
output = lboost(x,y,
               nlambda = 100,
               lf = 0,
               uf = 2,
               nu = 0.1,
               bstop = 100,
               nb = 5)
}
}
